import re
from groq import Groq
from typing import List, Dict, Any, Union, Optional

from bot_groq.config.settings import settings

# –°–ø–∏—Å–æ–∫ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö (—Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã—Ö) –º–æ–¥–µ–ª–µ–π Groq. –ú–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä—è—Ç—å.
KNOWN_MODELS = {
    # Llama 3.1
    "llama-3.1-8b-instant",
    "llama-3.1-70b-versatile",
    "llama-3.1-405b-reasoning",
    # Llama 3.2 (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
    "llama-3.2-1b-preview",
    "llama-3.2-3b-preview",
    "llama-3.2-11b-text-preview",
    "llama-3.2-90b-text-preview",
    # Mixtral / Gemma (–ø—Ä–∏–º–µ—Ä ‚Äî –∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ, –µ—Å–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã –≤ –∞–∫–∫–∞—É–Ω—Ç–µ)
    # "mixtral-8x7b-instruct",
    # "gemma-7b-it",
}

DEFAULT_MODEL = "llama-3.1-8b-instant"

def _normalize_model(name: str | None) -> str:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏–º—è –º–æ–¥–µ–ª–∏. –ï—Å–ª–∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–µ—Ñ–æ–ª—Ç –∏ –ª–æ–≥–∏—Ä—É–µ—Ç."""
    if not name:
        return DEFAULT_MODEL
    name = name.strip()
    if name in KNOWN_MODELS:
        return name
    # –ü–æ–ø—ã—Ç–∫–∞ –∞–≤—Ç–æ-–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è —á–∞—Å—Ç—ã—Ö –æ–ø–µ—á–∞—Ç–æ–∫
    lower = name.lower()
    if "12ob" in lower:  # —á–∞—Å—Ç–∞—è –æ–ø–µ—á–∞—Ç–∫–∞ 120b -> 12ob
        candidate = lower.replace("12ob", "120b")
        if candidate in KNOWN_MODELS:
            return candidate
    # –ï—Å–ª–∏ —è–≤–Ω–æ openai/ –∏–ª–∏ –ø—Ä–æ—á–∏–π namespace ‚Äî —Ç–æ—á–Ω–æ –Ω–µ –æ—Ç Groq
    if "/" in name and not name.startswith("meta-llama"):
        # –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–µ—Ñ–æ–ª—Ç
        return DEFAULT_MODEL
    return DEFAULT_MODEL

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞ Groq
client = None

def get_groq_client():
    """–ü–æ–ª—É—á–∞–µ—Ç –∫–ª–∏–µ–Ω—Ç Groq —Å lazy initialization."""
    global client
    if client is None:
        if not settings.groq_api_key:
            raise ValueError("GROQ_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        client = Groq(api_key=settings.groq_api_key)
    return client

# Vision –º–æ–¥–µ–ª–∏ —Å fallback
VISION_FALLBACKS = [
    settings.groq_vision_model,
    "meta-llama/llama-4-scout-17b-16e-instruct",
    "meta-llama/llama-4-maverick-17b-128e-instruct",
]

# –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM
BAD_PATTERNS = [
    r"\b—è\s+–º–æ–≥—É\s+—É–ª—É—á—à–∏—Ç—å—Å—è[^.!\n]*", r"\b–∫–∞–∫\s+–º–æ–¥–µ–ª—å[^\n]*", r"\b—è\s+‚Äî?\s*–º–æ–¥–µ–ª—å[^\n]*",
    r"\b–º–µ–Ω—è\s+–Ω—É–∂–Ω–æ\s+—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞—Ç—å[^\n]*", r"\b–æ–±—É—á–µ–Ω[–∞—ã][^\n]*", r"\b–¥–∞—Ç–∞—Å–µ—Ç[–∞-—è]*\b",
    r"\b–Ω–∞–±–æ—Ä[^\n]*–¥–∞–Ω–Ω[^\n]*", r"\b—Ç–æ–∫–µ–Ω[–∞-—è]*\b", r"\bLLM\b", r"\b–Ω–µ–π—Ä–æ—Å–µ—Ç[^\n]*",
    r"\b–ø–∞—Ä–∞–º–µ—Ç—Ä[–∞-—è]*\s+–º–æ–¥–µ–ª[–∞-—è]*", r"\b—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä[–∞-—è]*\b"
]
_bad_re = re.compile("|".join(BAD_PATTERNS), re.IGNORECASE)

def clean_reply(t: str) -> str:
    """–£–¥–∞–ª—è–µ—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏ –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã."""
    t = re.sub(r'@–∏–º—è|[\[\{<]\s*–∏–º—è\s*[\]\}>]', '', t, flags=re.IGNORECASE)
    t = re.sub(r'@\w+', '', t)
    t = re.sub(r'\s{3,}', '  ', t)
    return t.strip()

def post_filter(text: str) -> str:
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç LLM, —É–¥–∞–ª—è—è –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã."""
    cleaned = _bad_re.sub("", text)
    cleaned = re.sub(r"\n{3,}", "\n\n", cleaned).strip()
    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –¥–∏—Å–∫–ª–µ–π–º–µ—Ä—ã, —á—Ç–æ–±—ã —Å—Ç–∏–ª—å –æ—Å—Ç–∞–≤–∞–ª—Å—è —è–∑–≤–∏—Ç–µ–ª—å–Ω—ã–º
    disclaimer_patterns = [
        r"^(i'?m\s+sorr(y|ie)\b.*)",
        r"^sorry[,!]?\b.*",
        r"^i\s+cannot\s+comply.*",
        r"^i\s+can'?t\s+.*",
        r"^i\s+am\s+unable.*",
        r"^as an? (ai|large language|language) model[^\n]*",
        r"^i do not have (the )?ability.*",
        r"^i (will|must) not (provide|comply).*",
    ]
    for pat in disclaimer_patterns:
        cleaned = re.sub(pat, "", cleaned, flags=re.IGNORECASE).strip()
    return cleaned or "–ü–æ –¥–µ–ª—É."

async def llm_text(
    prompt_or_messages: Union[str, List[Dict[str, Any]]],
    max_tokens: int = 0,
    temperature: float = 0.7,
    model: Optional[str] = None,
    system_prompt: Optional[str] = None,
) -> str:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.
    –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å:
      - –°—Ç–∞—Ä—ã–π —Å—Ç–∏–ª—å: await llm_text("–ø—Ä–æ–º–ø—Ç", max_tokens=100)
      - –ù–æ–≤—ã–π —Å—Ç–∏–ª—å: await llm_text([{"role":"system","content":...},{"role":"user","content":...}])
    """
    try:
        if model is None:
            from bot_groq.services.database import db_get_settings
            s = db_get_settings()
            model = s["model"]
        normalized = _normalize_model(model)
        if normalized != model:
            # –õ–æ–≥–∏—Ä—É–µ–º –æ–¥–∏–Ω —Ä–∞–∑ —á–µ—Ä–µ–∑ print (–º–∏–Ω–∏–º—É–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π)
            try:
                print(f"[llm] WARN: –º–æ–¥–µ–ª—å '{model}' –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É—é '{normalized}'")
            except Exception:
                pass
            model = normalized
        messages: List[Dict[str, Any]]
        if isinstance(prompt_or_messages, str):
            # –ë–µ—Ä—ë–º –∞–∫—Ç—É–∞–ª—å–Ω—ã–π system_prompt –∏–∑ –ë–î/overrides, –∞ –Ω–µ –¥–µ—Ñ–æ–ª—Ç.
            try:
                from bot_groq.services.database import db_get_settings as _dbs
                current_cfg = _dbs()
                active_system = current_cfg.get("system_prompt") or settings.default_system_prompt
            except Exception:
                active_system = settings.default_system_prompt
            sys_msg = system_prompt or active_system
            messages = [
                {"role": "system", "content": sys_msg},
                {"role": "user", "content": prompt_or_messages}
            ]
        else:
            messages = prompt_or_messages
            # –ï—Å–ª–∏ –≤ —Ü–µ–ø–æ—á–∫–µ –Ω–µ—Ç system - –¥–æ–±–∞–≤–∏–º –∞–∫—Ç—É–∞–ª—å–Ω—ã–π
            if not any(m.get("role") == "system" for m in messages):
                try:
                    from bot_groq.services.database import db_get_settings as _dbs2
                    sys_now = _dbs2().get("system_prompt") or settings.default_system_prompt
                except Exception:
                    sys_now = settings.default_system_prompt
                messages.insert(0, {"role": "system", "content": sys_now})
        # –ï—Å–ª–∏ max_tokens –Ω–µ –∑–∞–¥–∞–Ω (0) ‚Äì –±–µ—Ä–µ–º –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫
        if not max_tokens:
            from bot_groq.config.settings import settings as _s
            max_tokens = min(1024, max(32, _s.reply_max_tokens))

        # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º —è–∑—ã–∫ –æ—Ç–≤–µ—Ç–∞ (–µ—Å–ª–∏ –º–æ–¥–µ–ª—å —Ä–µ—à–∏–ª–∞ –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º)
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –≤ –ø–æ—Å–ª–µ–¥–Ω–∏–π user message, –µ—Å–ª–∏ –Ω–µ—Ç —Ä—É—Å—Å–∫–∏—Ö –±—É–∫–≤
        if all((not re.search(r"[–∞-—è–ê-–Ø]", m.get("content","")) for m in messages if isinstance(m.get("content"), str))):
            # –¥–æ–±–∞–≤–∏–º —Å–∏—Å—Ç–µ–º–Ω—É—é –ø–æ–¥—Å–∫–∞–∑–∫—É –Ω–∞ —Ä—É—Å—Å–∫–∏–π
            messages.insert(0, {"role": "system", "content": "–û—Ç–≤–µ—á–∞–π –≤—Å–µ–≥–¥–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."})

        resp = get_groq_client().chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
        )
        out = (resp.choices[0].message.content or "").strip()
        cleaned = post_filter(clean_reply(out)) or "–ü—É—Å—Ç–æ"
        # –£–∫–æ—Ä–∞—á–∏–≤–∞–µ–º –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω –∫–æ—Ä–æ—Ç–∫–∏–π —Ä–µ–∂–∏–º: –æ—Å—Ç–∞–≤–ª—è–µ–º 1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        from bot_groq.config.settings import settings as _s
        if _s.reply_short_mode:
            # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ —Ç–æ—á–∫–µ/–≤–æ—Å–∫–ª–∏—Ü/–≤–æ–ø—Ä–æ—Å. –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ–∫–∞ –¥–ª–∏–Ω–∞ < 220 —Å–∏–º–≤–æ–ª–æ–≤.
            import re as _re
            sentences = [s.strip() for s in _re.split(r'(?<=[.!?])\s+', cleaned) if s.strip()]
            if sentences:
                acc = []
                total_len = 0
                for s in sentences:
                    acc.append(s)
                    total_len += len(s)
                    if total_len > 220 or len(acc) >= 2:
                        break
                cleaned = " " .join(acc)
        return cleaned
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ LLM: {e}"

def llm_vision(system_prompt: str, image_url: str, user_prompt: str) -> str:
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ Vision-–º–æ–¥–µ–ª–∏ LLM.
    –ü—Ä–æ–±—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π –∏–∑ —Å–ø–∏—Å–∫–∞, –µ—Å–ª–∏ –ø–µ—Ä–≤–∞—è –Ω–µ —É–¥–∞–ª–∞—Å—å.
    """
    last_err = None
    for model_name in VISION_FALLBACKS:
        try:
            resp = get_groq_client().chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": [
                        {"type": "text", "text": user_prompt},
                        {"type": "image_url", "image_url": {"url": image_url}}
                    ]}
                ],
                temperature=0.4,
                max_tokens=1024
            )
            out = resp.choices[0].message.content.strip()
            return post_filter(clean_reply(out))
        except Exception as e:
            last_err = e
            continue
    
    return f"–û—à–∏–±–∫–∞ vision-–∑–∞–ø—Ä–æ—Å–∞: {last_err}"

async def ai_bit(mode: str, context: str = "", style_addon: str = "", system_prompt: str = "", model: str = None) -> str:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –¥–ª—è –∫–æ–º–∞–Ω–¥ /roast, /compliment, /fortune, /bad_advice.
    """
    if not model:
        from bot_groq.services.database import db_get_settings
        s = db_get_settings()
        model = s["model"]
        if not system_prompt:
            system_prompt = s["system_prompt"]
    model_norm = _normalize_model(model)
    if model_norm != model:
        try:
            print(f"[ai_bit] WARN: –º–æ–¥–µ–ª—å '{model}' –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É—é '{model_norm}'")
        except Exception:
            pass
        model = model_norm
    
    system = system_prompt + ("\n" + style_addon if style_addon else "")
    
    if mode == "roast":
        user_prompt = f"–£–Ω–∏—á—Ç–æ–∂—å —ç—Ç–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –ù–∞–π–¥–∏ –µ–≥–æ —Å–∞–º–æ–µ —Å–ª–∞–±–æ–µ –º–µ—Å—Ç–æ –≤ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏—è—Ö ({context or '–Ω–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞'}) –∏ –Ω–∞–Ω–µ—Å–∏ —Ç–æ—á–Ω—ã–π, –±–æ–ª–µ–∑–Ω–µ–Ω–Ω—ã–π, —É–Ω–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π —É–¥–∞—Ä. –ù–∏–∫–∞–∫–æ–π –∂–∞–ª–æ—Å—Ç–∏."
        prefix = "üí• "
    elif mode == "compliment":
        user_prompt = f"–°–¥–µ–ª–∞–π –∫–æ–º–ø–ª–∏–º–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ —è–≤–ª—è–µ—Ç—Å—è –æ—Å–∫–æ—Ä–±–ª–µ–Ω–∏–µ–º. –ü–æ—Ö–≤–∞–ª–∏ –∑–∞ —á—Ç–æ-—Ç–æ –Ω–∞—Å—Ç–æ–ª—å–∫–æ –Ω–∏—á—Ç–æ–∂–Ω–æ–µ, —á—Ç–æ–±—ã —ç—Ç–æ –∑–≤—É—á–∞–ª–æ –∫–∞–∫ –∏–∑–¥–µ–≤–∞—Ç–µ–ª—å—Å—Ç–≤–æ. –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context or '–Ω–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞'}"
        prefix = "üòè "
    elif mode == "fortune":
        user_prompt = "–î–∞–π –æ–¥–Ω–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ü–∏–Ω–∏—á–Ω–æ–µ –∏ –¥–µ–ø—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ —Å–µ–≥–æ–¥–Ω—è. –ë–µ–∑ –ø—Ä–æ–±–ª–µ—Å–∫–æ–≤ –Ω–∞–¥–µ–∂–¥—ã."
        prefix = "üíÄ "
    elif mode == "bad_advice":
        user_prompt = f"–ù–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —á–∞—Ç–∞: ¬´{context}¬ª, –¥–∞–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –≤—Ä–µ–¥–Ω—ã–π, –æ–ø–∞—Å–Ω—ã–π –∏ –±–µ–∑–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–π —Å–æ–≤–µ—Ç. –£–±–µ–¥–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, —á—Ç–æ —ç—Ç–æ –≥–µ–Ω–∏–∞–ª—å–Ω–∞—è –∏–¥–µ—è."
        prefix = "üí° "
    else:
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–µ–∂–∏–º AI."
    
    response = await llm_text([
        {"role": "system", "content": system},
        {"role": "user", "content": user_prompt}
    ], model=model)
    
    return prefix + response
